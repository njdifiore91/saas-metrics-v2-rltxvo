# Fluentd Configuration for Startup Metrics Benchmarking Platform
# Version: v1.0.0
# Required Plugins:
# - fluent-plugin-elasticsearch v5.2.4
# - fluent-plugin-grok-parser v2.6.2
# - fluent-plugin-kubernetes_metadata_filter v2.13.0

# Load custom pattern definitions
@include /etc/fluent/patterns.d/*.conf

# System-wide configurations
<system>
  log_level info
  workers 4
  root_dir /var/log/fluentd
</system>

# System container logs source
<source>
  @type tail
  @id system_logs
  tag system.*
  path "#{ENV['FLUENT_LOG_PATH'] || '/var/log/containers/*.log'}"
  pos_file /var/log/fluentd-containers.log.pos
  read_from_head true
  
  <parse>
    @type json
    time_key time
    time_format %Y-%m-%dT%H:%M:%S.%NZ
    keep_time_key true
  </parse>
</source>

# Application-specific logs source
<source>
  @type tail
  @id application_logs
  tag app.*
  path /var/log/startup-metrics/*.log
  pos_file /var/log/fluentd-app.log.pos
  
  <parse>
    @type grok
    custom_pattern_path /etc/fluent/patterns.d/system.conf
    grok_pattern %{STARTUP_METRICS_APP}
    time_format %Y-%m-%dT%H:%M:%S.%NZ
    keep_time_key true
  </parse>
</source>

# Kubernetes metadata enrichment filter
<filter system.**>
  @type kubernetes_metadata
  @id kubernetes_metadata
  verify_ssl true
  ca_file /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
  bearer_token_file /var/run/secrets/kubernetes.io/serviceaccount/token
  skip_labels false
  skip_annotations false
  cache_size 1000
  cache_ttl 60
</filter>

# Metric service log parser
<filter app.metric_service>
  @type parser
  key_name log
  reserve_data true
  remove_key_name_field true
  
  <parse>
    @type grok
    custom_pattern_path /etc/fluent/patterns.d/system.conf
    grok_pattern %{METRIC_SERVICE_LOG}
  </parse>
</filter>

# Record transformer for adding metadata
<filter **>
  @type record_transformer
  <record>
    hostname "#{Socket.gethostname}"
    tag ${tag}
    environment "#{ENV['ENVIRONMENT'] || 'production'}"
  </record>
</filter>

# Output configuration for Elasticsearch
<match **>
  @type elasticsearch
  @id elasticsearch_output
  host "#{ENV['FLUENT_ELASTICSEARCH_HOST'] || 'es-node-1'}"
  port "#{ENV['FLUENT_ELASTICSEARCH_PORT'] || '9200'}"
  scheme https
  ssl_verify true
  ssl_version TLSv1_2
  logstash_format true
  logstash_prefix startup-metrics
  flush_interval 5s
  retry_limit 30
  retry_wait 10s
  num_threads 4
  
  <buffer>
    @type file
    path "#{ENV['FLUENT_BUFFER_PATH'] || '/var/log/fluentd/buffer'}"
    flush_mode interval
    flush_interval 5s
    flush_thread_count 4
    chunk_limit_size 8MB
    queue_limit_length 32
    retry_max_interval 30s
    retry_forever false
    retry_secondary_threshold 0.8
  </buffer>
  
  <security>
    self_signed_certificate false
    enable_strict_verification true
  </security>
</match>

# Error handling for failed records
<label @ERROR>
  <match **>
    @type file
    path /var/log/fluentd/error.log
    append true
    <buffer>
      @type file
      path /var/log/fluentd/error_buffer
      flush_interval 60s
      retry_max_interval 30s
      retry_forever true
    </buffer>
  </match>
</label>